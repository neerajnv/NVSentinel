# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

replicaCount: 1

image:
  repository: ghcr.io/nvidia/nvsentinel/preflight
  pullPolicy: IfNotPresent
  tag: "latest"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

securityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 65532
  runAsGroup: 65532
  capabilities:
    drop: ["ALL"]

service:
  type: ClusterIP
  port: 443
  targetPort: 8443

resources:
  requests:
    cpu: 250m
    memory: 512Mi
  limits:
    cpu: 500m
    memory: 1024Mi

livenessProbe:
  httpGet:
    scheme: HTTPS
    path: /healthz
    port: 8443
  initialDelaySeconds: 5
  periodSeconds: 10

readinessProbe:
  httpGet:
    scheme: HTTPS
    path: /healthz
    port: 8443
  initialDelaySeconds: 2
  periodSeconds: 5

nodeSelector: {}
tolerations: []
affinity: {}

webhook:
  port: 8443
  failurePolicy: Fail
  timeoutSeconds: 10
  # Set to true to create the two-tier CA
  # Set to false to use an existing CA issuer
  createIssuer: true
  # Only used when createIssuer=false
  certIssuer: ""
  caCertificateName: ""

# Local fallback values - global.dcgm takes precedence when set
dcgm:
  service:
    # DCGM hostengine service endpoint (fallback when global.dcgm.service not set)
    endpoint: "nvidia-dcgm.gpu-operator.svc"
    port: 5555
  # Diagnostic level:
  #   1 = Quick (~30 seconds) - basic health check
  #   2 = Medium (~2 minutes) - more thorough
  #   3 = Long (~15 minutes) - comprehensive
  diagLevel: 2
  # Event processing strategy: EXECUTE_REMEDIATION or STORE_ONLY
  processingStrategy: "EXECUTE_REMEDIATION"

initContainers:
  - name: preflight-dcgm-diag
    image: ghcr.io/nvidia/nvsentinel/preflight-dcgm-diag:latest
    volumeMounts:
      - name: nvsentinel-socket
        mountPath: /var/run
    # Full corev1.Container fields supported:
    # resources:
    #   limits:
    #     memory: 512Mi

  # # NCCL loopback test - validates intra-node GPU-to-GPU communication (NVLink/PCIe)
  - name: preflight-nccl-loopback
    image: ghcr.io/nvidia/nvsentinel/preflight-nccl-loopback:latest
    env:
      - name: BW_THRESHOLD_GBPS
      # Minimum acceptable bus bandwidth
      # Valid for NVLink GPU-to-GPU interconnect
      # If using PCIe GPU-interconnect, this threshold should be set to ~15 GB/s
        value: "150"  # Minimum acceptable bus bandwidth in GB/s
      - name: TEST_SIZE_MB
        value: "256"  # Message size for all-reduce test
      # To skip the bandwidth check, set SKIP_BANDWIDTH_CHECK to true
      # By default, the bandwidth check is enabled
      # Checks if the loopback test passes without checking the bandwidth
      # - name: SKIP_BANDWIDTH_CHECK
      #   value: "true" 
    volumeMounts:
      - name: nvsentinel-socket
        mountPath: /var/run

  # NCCL all-reduce test - validates multi-node GPU communication
  # Requires gangCoordination.enabled=true and gang-aware scheduler (Volcano, Kueue, etc.)
  - name: preflight-nccl-allreduce
    image: ghcr.io/nvidia/nvsentinel/preflight-nccl-allreduce:latest
    securityContext:
      capabilities:
        add: ["IPC_LOCK"]  # Required for RDMA memory registration
    env:
      # Benchmark configuration (fabric-agnostic defaults)
      - name: BW_THRESHOLD_GBPS
        value: "100"  # Minimum acceptable bus bandwidth in GB/s
      - name: MESSAGE_SIZES
        value: "4G"  # Message size for bandwidth test
      - name: NCCL_DEBUG
        value: "INFO"  # Set to WARN after transport selection is validated
      - name: NCCL_DEBUG_SUBSYS
        value: "INIT,NET"
    volumeMounts:
      - name: nvsentinel-socket
        mountPath: /var/run

# Fabric-specific NCCL configuration.
#
# In production, the webhook automatically copies NCCL env vars and volume
# mounts from the user's main container to preflight init containers using
# ncclEnvPatterns and volumeMountPatterns. No manual fabric config needed.
#
# For standalone testing (busybox main container), use ncclAllreduceExtraEnv
# and gangCoordination.extraHostPathMounts to provide fabric config explicitly.
#
# Tested fabric configurations (for reference):
#
#   InfiniBand (Azure NDv4/v5, 166 GB/s):
#     Env: NCCL_TOPO_FILE=/etc/nccl/topo.xml, NCCL_IB_PCI_RELAXED_ORDERING=1,
#          NCCL_SOCKET_IFNAME=eth0
#     Mounts: ncclTopoShape=ndv4 (auto-creates topo ConfigMap)
#
#   EFA (AWS H100, 346 GB/s):
#     Env: FI_EFA_USE_DEVICE_RDMA=1, FI_PROVIDER=efa, NCCL_SOCKET_IFNAME=eth0,
#          LD_LIBRARY_PATH=/opt/amazon/ofi-nccl/lib:/opt/amazon/efa/lib:...
#     Mounts: /opt/amazon from host; libefa.so.1 + libibverbs.so.1 bind-mounts
#     Note: Dockerfile removes /opt/amazon to avoid ABI mismatch with host EFA
#
#   MNNVL + EFA (AWS GB200, 689 GB/s):
#     Env: FI_EFA_USE_DEVICE_RDMA=1, NCCL_MNNVL_ENABLE=1, NCCL_NVLS_ENABLE=0,
#          NCCL_NET_GDR_LEVEL=PHB, NCCL_P2P_NET_CHUNKSIZE=2097152
#     Mounts: /opt/amazon-efa-ofi from host
#     Requires: ComputeDomain DRA claim, Volcano DRA feature gate
#
#   TCPXO (GCP H100 A3+, 331 GB/s):
#     Env: NCCL_SOCKET_IFNAME=eth1-8, NCCL_FASTRAK_*, NCCL_BUFFSIZE=8388608,
#          NCCL_SHIMNET_GUEST_CONFIG_CHECKER_CONFIG_FILE, NCCL_TUNER_PLUGIN, etc.
#     Mounts: nvtcpxo-libraries → /usr/local/nvidia,
#             nvtcpxo-aperture-devices → /dev/aperture_devices (from GKE webhook)

# Glob patterns for environment variable names to copy from the user's main
# containers to preflight init containers. Allows automatic inheritance of
# fabric-specific NCCL config without manual configuration.
ncclEnvPatterns:
  - "NCCL_*"
  - "FI_*"
  - "LD_LIBRARY_PATH"
  - "UCX_*"
  - "TORCH_NCCL_*"
  - "CUDA_DEVICE_ORDER"

# Glob patterns for volume mount names to copy from the user's main
# containers to preflight init containers. Allows automatic inheritance of
# fabric-specific host mounts (EFA libs, TCPXO plugins, etc.).
volumeMountPatterns:
  - "host-opt-amazon*"
  - "host-libefa*"
  - "host-libibverbs*"
  - "amazon-efa*"
  - "nvtcpxo-*"
  - "nccl-*"
  - "dev-shm"

# Additional env vars appended to the preflight-nccl-allreduce init container
# AFTER user-container env vars are copied. Use for standalone testing or
# to override specific values.
ncclAllreduceExtraEnv: []

gpuResourceNames:
  - "nvidia.com/gpu"

# Network resource names to copy to init containers (RDMA, InfiniBand, etc.)
networkResourceNames:
  - "nvidia.com/mlnxnics"
  - "vpc.amazonaws.com/efa"
  # GCP multi-network resources (TCPXO)
  - "networking.gke.io.networks/gpu-nic0"
  - "networking.gke.io.networks/gpu-nic0.IP"
  - "networking.gke.io.networks/gpu-nic1"
  - "networking.gke.io.networks/gpu-nic1.IP"
  - "networking.gke.io.networks/gpu-nic2"
  - "networking.gke.io.networks/gpu-nic2.IP"
  - "networking.gke.io.networks/gpu-nic3"
  - "networking.gke.io.networks/gpu-nic3.IP"
  - "networking.gke.io.networks/gpu-nic4"
  - "networking.gke.io.networks/gpu-nic4.IP"
  - "networking.gke.io.networks/gpu-nic5"
  - "networking.gke.io.networks/gpu-nic5.IP"
  - "networking.gke.io.networks/gpu-nic6"
  - "networking.gke.io.networks/gpu-nic6.IP"
  - "networking.gke.io.networks/gpu-nic7"
  - "networking.gke.io.networks/gpu-nic7.IP"

# Gang discovery configuration for multi-node preflight checks.
# Default (empty): K8s 1.35+ native WorkloadRef API
# For PodGroup-based schedulers, set name and other fields:
gangDiscovery: {}
  # name: "volcano"
  # annotationKeys: ["scheduling.k8s.io/group-name"]
  # labelKeys: []  # optional fallback
  # podGroupGVR:
  #   group: "scheduling.volcano.sh"
  #   version: "v1beta1"
  #   resource: "podgroups"
  # minCountExpr: "podGroup.spec.minMember"  # CEL expression

# Gang coordination configuration for multi-node checks (e.g., nccl-allreduce)
gangCoordination:
  # Enable gang coordination (required for gang-wide checks)
  enabled: true
  # Maximum time to wait for all gang members to register
  timeout: "10m"
  # Port for PyTorch distributed TCP bootstrap (torchrun)
  masterPort: 29500
  # Path where gang ConfigMap is mounted in init containers
  configMapMountPath: "/etc/preflight"
  # NCCL topology ConfigMap name — required for Azure InfiniBand.
  # Without this, NCCL cannot map GPUs to IB HCAs and falls back to TCP.
  # Set explicitly to use a pre-existing ConfigMap, or use ncclTopoShape
  # to auto-create one from the chart's bundled topology files.
  ncclTopoConfigMap: ""
  # VM shape for auto-creating the NCCL topology ConfigMap (IB only).
  # Supported: "ndv4", "ndv5" (files shipped in charts/preflight/files/).
  # When set, a ConfigMap is created from files/<shape>-topo.xml and
  # ncclTopoConfigMap is auto-resolved to its name.
  ncclTopoShape: ""

  # Optional hostPath mounts injected into gang-aware preflight init containers.
  # Fabric-specific; set via overlay files (e.g. values-mnnvl-efa.yaml).
  extraHostPathMounts: []

  # Mount pre-existing pod volumes into preflight init containers.
  # Does NOT create new volumes — only adds volumeMounts for volumes already
  # present in the pod (e.g. injected by a platform webhook).
  # Fabric-specific; set via overlay files (e.g. values-tcpxo.yaml).
  extraVolumeMounts: []

  # Automatically mirror pod-level DRA resource claims (spec.resourceClaims)
  # to preflight init containers' resources.claims. This ensures init
  # containers get the same device access as main containers: GPUs, RDMA
  # NICs, IMEX channels (GB200 MNNVL), etc. See ADR-026 §DRA Integration.
  # Defaults to true when omitted.
  # mirrorResourceClaims: true

# Label-based namespace selection
# Enable preflight in a namespace: kubectl label namespace <name> nvsentinel.nvidia.com/preflight=enabled
namespaceSelector:
  matchLabels:
    nvsentinel.nvidia.com/preflight: "enabled"

networkPolicy:
  enabled: true
